#### Ch1. Introduction
- Natural Language Processing(NLP), also called Computational Linguistics
- **sequence of symbols의 처리**, input은 ‘의미가 있는 symbol string’, ‘전달하고자 하는 내용의 representation’
- “입력 문장 -> 형태소 분석기 -> 구문 분석기(w/문법) -> 의미 분석기 -> 담화 분석기 -> 문장 생성기(w/생성사전, 생성문법) -> 출력문장” , 사전, 각종 지식기반
- Phonetics/Phonology/음운론 -> Morphology형태론 -> Syntax통사론 -> Semantics의미론 -> Pragmatics화용론 -> Discourse담화론
- Sequence of symbols -> 의미있는 token -> 품사 정보 labeling -> Phrase Chunking -> parse tree(token간 관계) -> 의미분석 -> 상황정보 
- Ambiguities everywhere! (Morphology level, V/N Ambiguity in syntax/word sense, Language Evolves, )

#### Ch2. 형태소 분석 Morphological Analysis
- 한국어 특성, 품사 분류/ 세종 태그 프로젝트, 좌우접속 정보, 용언의 불규칙 활용과 음운 현상
- ISAM을 이용한 사전 검색, TRIE이용 사전 검색
- 형태소 분석기 구현: Tabular Parsing

#### Ch3. Grammars and Parsing
1. Grammar 문법: 문장의 구조적 성질을 규칙으로 표현한 것 **A set of rewrite rules**, Context Free Grammar(CFG)
2. Syntactic Parsing 구문 분석기: 문법을 이용하여 문장의 구조를 찾아내는 process, Tree형태로 표현. 몇 개의 형태소 -> 구문 요소(구: phrase) -> 구문 요소들간의 결합구조: Tree형태로 최종 구문 구조; S> NP, VP > NP+PP, Pro, V, Det+N, Prep+NP 등 // Produces all possible parse trees -> Syntactic Ambiguity 발생
    1. Given 1) a string of terminals and 2)a CFG, determine if the string can be generated by the CFG / Top-Down, Bottom-up, Caching(Memoizing)
    2. Dynamic Programming Parsing Methods: **CKY Parser**(First grammar -> Chomsky normal form(CNF), O(n^2) cells, O(n) split points —> O(n^3) in the end), Earley’s parser, Tomita’s Parser(LR Parser)

#### Ch4. Language Modeling with N-grams
- Conditional Probability, Language Modeling, Markov Assumption —> N-Gram Models
- Maximum Likelihood Estimation
- Problems 1) the perils of overfitting, 2) zeros of not? Zipf’s law
- ** Smoothing** 1) Robin Hood, 2) Laplace(Add-one), 3) Add-k +a) Backoff, Interpolation
- Evaluation: **Perplexity** an intrinsic evaluation measure of how well a model “fits” the test data; the probability of the test set(assigned by the language model), normalized by the number of words. Minimize perplexity == Maximizing probability == Better model
- Unknown words: Out Of Vocabulary(OOV) <UNK>

#### Ch5. POS Tagging, Sequence Labeling, and HMM
1. Part Of Speech Tagging: the lowest level of syntactic analysis. —>는 ambiguity 발생 -> probability를 활용하자(HMM)
2. Sequence Labeling
3. Hidden Markov Models: A set of states, Transition probabilities, Distinguished start and end states
    1. Decoding: Forward Algorithm, The Viterbi algorithm
4. Baum-Welch algorithm/ EM algorithm

#### Ch6. Statistical Parsing
- Phrase Chunking as Sequence Labeling: B(begin)I(inside)O(other) tagging
- Probabilistic(P)CKY Parser
- 한국어 구문 분석: Dependency Grammars 의존문법을 이용 1) 투영의 원칙(엇갈린 의존관계X) 2) 지배성분(Head) 유일의 원칙(임의의 문장 성분은 다른 하나의 문장 성분만을 수식) 3) 지배성분 후위의 원칙

#### Ch7. Neural Shift-Reduce Dependency Parsing
- Shift-Reduce dependency parser
- Shift: Push the next word in the buffer onto the stack
- Reduce: Replace a set of the top elements on the stack with a constituent composed of them,
