#### Ch8. Neural Networks and Neural Language Models + Word Vector
1. Basic Concepts of Neural Network
    1. Neural Network, Perceptron
    2. Learning: Back propagation
2. Deep Learning: Auto Encoder, Learning Representation for NLP: feature vector, one-hot encoding, distributed representation(유사한 것은 유사하게 표현), word embedding(Ranking-based, word2vec(skipgram, CBOW), Glove) + negative sampling

#### Ch9. LSTM RNNs and Sequential Labeling
- RNNs -> Unrolled RNN -> Training RNNs, Bidirectional RNNs
- Vanishing/Exploding Gradient Problem: Cannot handle long distance dependencies => Use LSTM!
- Long Short Term Memory (LSTM): Cell state(retain state information longer), Forget gate, Input gate, Output gate => Bidirectional LSTM 이 대세 
- GRU: No cell state, Forget + Input gate —> Update gate; LSTM과 비교해 parameter가 훨씬 적다는 장점, 많이 쓰이진 않음
- LSTM의 활용 예: Named Entity Recognition (NER), CoNLL 포맷 Sequential Labeling(POS tagging, NER BIO tagging)

#### Ch10. Semantic Processing 의미해석
1. Word sense Disambiguation: 모호한 구문 해석(구문 해석은 되나 의미 해석이 안되는, ex. 비행기가 사과를 먹는다)
2. Case Grammar: 각 문장의 head meaning이 ** 용언**에 있음. 다른 성분들은 용언(술부;predicate)에 대한 case(semantic role, 의미격;)으로 표현.  ex) agent, object, instrument, beneficiary, experiencer, loc, time…… Case(Semantic role)만으로는 부족, Selectional Restrictions사용 (world knowledge, taxonomic abstraction hierarchies or ontologies의 영역)
3. Word 1)WordNet: synsets(wordnet의 기본 단위;개념적인 의미가 같은것). Antonym, Attribute, Pertainym, Similar, Cause, Entailment… 2) Word-sense Disambiguation; POS 추측, classification problem(의미1 or 의미2?), 
4. Semantic Role Labeling(=Case analysis). 기존엔 rule-based, but empirically done nowadays as Sequence Labeling.
5. Semantic Grammar: selectional restriction과 단어 feature등을 추가해서 처리한 문법. 문법적으로 맞지만 의미적으로 틀린 명사구(word sense disambiguation)를 잡아낼 수 있다. 제한된 간단한 영역에서 제한된 의미 범주를 사용하는 것이 단순

#### Ch11. Vector Space(Distributional) Lexical Semantics
- knowledge map 을 어떻게 잘 만들어야 하지? 1) Ontology 2) Word vector(distributional representation)
- Vector-space Model: Term Frequency-Inverse Document Frequency(TF-IDF) Weighting
- Similarity Measure 1) Cosine similarity between word vectors on lexical vector space 2)Vector Semantics Computations; king = queen - woman + man(word analogies)

#### Ch12. Discourse Analysis
1. Reference Resolution ex) “one”이나 “some”이 참조하는 object를 어떻게 찾아 내는가?
2. Text Understanding with World Knowledge: Causality 1)Relation between Actions and State: Effect Causality, Precondition Causality 2)Relations between two Actions: Enablement, Decomposition, Generation
3. Attentional states: 1) 근거(ex.시제)를 가지고 문단 나누기 2) 문단간의 관계 찾기 \\ Focus History Stack: 

#### Ch13. Conversational Agent: Dialog Interface and ChatBot
- 대화형 사용자 인터페이스의 중요성 대두
- 채팅(1문1답, ChatBot), 대화(Multi-turn, Dialogue, General goal-oriented dialogue), Q/A(Question/Answering, 방대한 text에서 답에 해당하는 문서 제공)
- Speech recognition -> Language understanding(nlp) -> 대화 관리자 & Data/ Knowledge base -> Language generation -> Speech synthesis
    - 자연어 이해: 사용자의 발화를 언어처리 통해 분석, 사용자의 의도를 표현하는 의미표현 생성
    - 대화 관리자: 대화 흐름, 상환 고려해서 사용자 발화 의도에 대한 최선의 대화 전략 결정, 다음 시스템 발화 생성
    - 자연어 생성: 입력된 의미 표현으로부터 시스템 발화 문장 생성
- 채팅 시스템: Eliza(말꼬리), 유사도 기반 검색(대화쌍 DB), Deep Learning기반(seq2seq), info-chatting(관련 정보 관련 DB)
- 발화 분석을 위한 자연어 이해
    - 문자열 -> 단어 (형태소 분석) -> (개체명 인식) -> (영역 검출) -> (화행Speech acts 분석)
    - 내일 인천 송도 날씨 어때? -> 내일/NN, 인천/NN, 송도/NN, 날씨/NN, 어때?/VC -> 시간, 장소, 장소 -> 날씨 -> AskRef(날씨)
- Dialogue 관리자가 고려해야 하는 대화 지식 1)Turn-taking and Utterance(Who&When, Overlap&Silence, 2)Initiative Strategies), 3)Grounding/ 4)Repairing misunderstandings(5)Confirmation Strategies)
- Conversational Agent
